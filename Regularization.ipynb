{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition for Regularization\n",
    "\n",
    "Okay, so you understand regression like a champion, but something tells you that these models are not a cure-all that explain all data. Big data sets are notorious for growing complex and creating extreme explanations for the data they describe. Even if these extreme explantions fit the data better than a simpler model, they are less likely to be  generalized. These models explain data from the past well, but are less likely to explain for future data it has never seen before. This issue, as you probably heard, is called an overfit model.\n",
    "\n",
    "Take this trivial example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>size</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bo</th>\n",
       "      <td>snake</td>\n",
       "      <td>small</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spot</th>\n",
       "      <td>dog</td>\n",
       "      <td>small</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo</th>\n",
       "      <td>cat</td>\n",
       "      <td>small</td>\n",
       "      <td>enemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lefty</th>\n",
       "      <td>cat</td>\n",
       "      <td>small</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Curly</th>\n",
       "      <td>dog</td>\n",
       "      <td>large</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom</th>\n",
       "      <td>snail</td>\n",
       "      <td>small</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jerry</th>\n",
       "      <td>dog</td>\n",
       "      <td>large</td>\n",
       "      <td>enemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sylvester</th>\n",
       "      <td>cat</td>\n",
       "      <td>large</td>\n",
       "      <td>enemy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          animal   size     label\n",
       "Bo         snake  small  friendly\n",
       "Spot         dog  small  friendly\n",
       "Mo           cat  small     enemy\n",
       "Lefty        cat  small  friendly\n",
       "Curly        dog  large  friendly\n",
       "Tom        snail  small  friendly\n",
       "Jerry        dog  large     enemy\n",
       "Sylvester    cat  large     enemy"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "index = ['Bo', 'Spot', 'Mo', 'Lefty', 'Curly', 'Tom', 'Jerry', 'Sylvester']\n",
    "columns = ['animal', 'size', 'label']\n",
    "data = np.array([['snake', 'dog', 'cat', 'cat', 'dog', 'snail', 'dog', 'cat'],\n",
    "                 ['small', 'small', 'small', 'small', 'large', 'small', 'large', 'large'], \n",
    "                ['friendly', 'friendly', 'enemy', 'friendly', 'friendly', 'friendly', 'enemy', 'enemy']]).T\n",
    "df = pd.DataFrame(data=data, index=index, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model builds the rule:\n",
    "\n",
    "    \"pets with up to four-letter names are enemies, as are large dogs with names beginning with 'J', except that small snakes are not enemies\"\n",
    "\n",
    "The model is dumb. Yes, it fits perfectly, but its clunky. Intuitively, we know this will not generalize out to all the pets in the real world. There must be a simpler rule. Consider this:\n",
    "\n",
    "    \"large dogs and cats are enemies\"\n",
    "\n",
    "It does not fit the data perfectly, but fits reasonably well. It's simpler, and we may suppose that this rule will better hold true to thousands more examples.\n",
    "\n",
    "Regularization prevents this overfitting. Loosely, regularization is a technique to discourage complexity and increase  generalization, and it sometimes comes at the cost of model accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Okay\", you say. I'm ready for level 2.\n",
    "\n",
    "### Underfitting \n",
    "First - a quick note on underfitting. This is the condition where our model is not able to find any relationship between inputs and outputs. Our model cannot find any corelation between a pet's characteristics and friendliness. Underfitting is intuitive to catch, just look at the classification accuracy on the training data. If the accuracy is low, the model may be too simple, and therefore underfit. Increase the complexity (the number of paramters in the model), and get a better fit. As we will see, regularization also helps alleviate this problem. \n",
    "\n",
    "### Overfitting \n",
    "On the other end of the spectrum, overfitting is more difficult to detect. You must monitor when your model is performing _too_ well on training data, and poorly on testing data. \n",
    "\n",
    "Regularization will help the model find a happy medium between these two states, and minimize the overall prediction error. It tweaks the model to avoid being too simple, avoid being too complex, and find something juuuuust right. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/bias_variance_tradeoff_e.jpg\">\n",
    "<center>** Figure 1 **</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization \n",
    "\n",
    "So how do we implement regularization? It is acheived by penalzing the weights in our model. Let's take a high level look at our model: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/model_matrix.png\" width='600'>\n",
    "<center>** Figure 2 **</center>\n",
    "\n",
    "In this infographic, there are four parameters to our model: \n",
    "\n",
    "    1) x: The input data with i (height) data points and j (width) features\n",
    "    2) y: The labels \n",
    "    3) ŷ: The predictions (what our model outputs)\n",
    "    4) W: The weights of the \"j\" coefficents\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/prediction_eq.png'>\n",
    "\n",
    "\n",
    "\n",
    "In mathematical terms, our prediction, ŷ, equals the weighted sum of the data points. \n",
    "\n",
    "### Penalizing Weights\n",
    "\n",
    "Regularization penalizes the weights, W, of our model, above. How does this help? In Figure 1 we learned that after a point, increasing model complexity is bad for the prediction accuracy. The model complexity causes overfitting by contouring to very small deviations in the data. It turns out that **the size of the coeffient weights, W, increases exponentially with model complexity**.  \n",
    "\n",
    "The magnitude of the coefficient represents the emphasis we are putting on that feature as a predictor. If we think animal size is important to friendliness, we will give that feature a large weight. When the weight becomes too large, the algorithm starts creating intricate relations to model the output, which is not desireable. Another way to think of it, is that features with large weights can almost signlehandedly control the output prediction. Think about that first model for animal friendliness (the dumb one). \n",
    "\n",
    "Penalizing the weights constrains them, and helps reduce model complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Ridge Regression\n",
    "\n",
    "Ridge Regresssion is the most common type of regularization penalty. It is calculated as the sum of the squares of the weights. Here we see the equation for the L2 penalty. Later I will cover how to implement it into the model. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/l2_eq.png'>\n",
    "\n",
    "we can gain an intuition of it in python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "penalty = 0 #initialzie to zero, R(W) in the equation above\n",
    "W = np.zeros(shape=(10,10)) #fake weights matrix\n",
    "\n",
    "for i in np.arange(0, W.shape[0]):\n",
    "    for j in np.arange(0, W.shape[1]):\n",
    "        penalty += (W[i][j]**2)#square the weights and add "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are looping over the entire weights matrix and summing the squares of the weights. We can do this more efficiently with linear algebra, but this is just for demonstration. As an exponential function, you can see that L2 will penalize large weights from our W matrix, and prefer smaller ones. This will seek out W values that represent all features of the data. \n",
    "\n",
    "We should note that L2 is primarily used to prevent overfitting. The result keeps all the features in the model, but may reduce their weight. This is not true of all types of regulariztion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Lasso Regression\n",
    "\n",
    "L1 takes the absolute value rather than the square:\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/l1_eq.png'>\n",
    "\n",
    "L1 regularization shrinks coefficients, like L2, but it may also reduce some coefficents to zero, which is unique. A coeficent of zero essentially means that feature has been removed from the model. This is useful in senarios where we have hundres or thousands of features, like image classification, but may be excessive for smaller needs. \n",
    "\n",
    "we can gain an intuition of L1 in python: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "penalty = 0 #initialzie to zero, R(W) in the equation above\n",
    "W = np.zeros(shape=(10,10)) #fake weights matrix\n",
    "\n",
    "for i in np.arange(0, W.shape[0]):\n",
    "    for j in np.arange(0, W.shape[1]):\n",
    "        penalty += (abs(W[i][j]))#absolute value the weights and add "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few other types of regularization (Elastic Net, Dropout) used for other models. The key takeway is that we have calculated an R(W) penalty term. \n",
    "\n",
    "# Loss Function\n",
    "\n",
    "We must now add this term to our loss function (also called cost function or objective function). The loss function maps our model performance to some \"real-world\" interpretation, where we can measure its error as a \"cost\". Our goal is to minimize that cost; minimize the error.  Quantitatively, the loss function answers the following: \n",
    "\n",
    "    \"The real label was 1, but I predicted 0. Is that bad?\" \n",
    "    \"Yeah. That's bad. 500 bad, to be specific.\"\n",
    "\n",
    "Or \"X bad\", where X is the measure of the cost. Mathematically, this is the most basic form of the loss function: \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/loss_func.png' width='150'>\n",
    "\n",
    "To implement regularization, we multiply our penalty term, R(W), by a coefficent λ, and add that  to the loss function. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/loss_func_add_reg.png' width='220'>\n",
    "\n",
    "λ is a hyperparamter, which means that it is a variable that _**we**_ set, unlike the weights, W, or predictions ŷ. We can also tune λ like a radio dial, and set it to value that gives us the best results. If cleaning up data is the thing you do most in data science, then tuning hyperparameters is the thing you do second most. \n",
    "\n",
    "And that's it! We update weights in the loss function, to minimize our prediction error. In doing this, we have reduced the magnitude of the coefficeints, and therefore reduced the model complexity. The trick comes in idenifying when and how to implement regularization. \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Regularization is a technique to optimize model fit, most commonly to prevent overfitting. It works by penalizing the weights matrix, W, based on the size of your feature coefficients. Applying the penalty has the effect of generalizing the model to new data, but sometimes lowering the training accuracy. L2 regularization, or Ridge, is the most common for preventing overfitting. L1 regularization has the possibility to remove features from the data, and is better for extremely large datasets. Regularization comes the need to tune the λ hyperparameter, which is the stregnth of the penalty. \n",
    "\n",
    "In practice, regularization has the following work flow:\n",
    "\n",
    "    1) Determine if regularization should be applied. Is there a discrepancy in training and testing accuracy? \n",
    "    2) Which type to use? Is the model overfit? How big is the dataset? \n",
    "    3) Tune λ, the strength of the regularization to optimize testing accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "1) https://www.quora.com/What-is-regularization-in-machine-learning\n",
    "\n",
    "2) http://www.pyimagesearch.com/2016/09/19/understanding-regularization-for-image-classification-and-machine-learning/\n",
    "\n",
    "3) https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
